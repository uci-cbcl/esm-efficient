rule finetune_meltome_train:
    input:
        model='data/results/models/1v/1v_1.safetensors',
        # model=model,
        dataset=config["meltome"]["dataset"],
    threads: 16
    params:
        devices=[3],
        epochs=100,
        checkpoint_dir=config["meltome"]["checkpoint_dir"].replace(
            "{lora}/", "{lora}_head={head}/"
        ),
    output:
        lora_weights=config["meltome"]["lora_weights"].replace(
            ".safetensors", "_head={head}.safetensors"
        ),
        head_weights=config["meltome"]["head_weights"].replace(
            ".safetensors", "_head={head}.safetensors"
        ),
    script:
        "finetune_meltome.py"


rule finetune_meltome_test:
    input:
        model=model,
        lora_weights=config["meltome"]["lora_weights"].replace(
            ".safetensors", "_head={head}.safetensors"
        ),
        head_weights=config["meltome"]["head_weights"].replace(
            ".safetensors", "_head={head}.safetensors"
        ),
        dataset=config["meltome"]["dataset"],
    threads: 16
    params:
        device=3,
    output:
        predictions=config["meltome"]["predictions"],
    notebook:
        "test_meltome.py.ipynb"


rule meltome_plot_scatter:
    input:
        predictions=config["meltome"]["predictions"],
    threads: 1
    output:
        scatter_test=f"reports/figures/meltome_scatter_test_{finetune_wildcards}_head={{head}}.svg",
    script:
        "scatter_meltome.py"


rule meltome_barplot:
    input:
        pred=expand(
            config["meltome"]["predictions"],
            model=["8Me", "35Me", "150Me", "650Me", "3Be"],
            lora=["none"],
            head=["nn"],
            quantize=["none"],
            checkpointing=True,
        ),
        pred_lora=expand(
            config["meltome"]["predictions"],
            model=["8Me", "35Me", "150Me", "650Me", "3Be"],
            head=["nn"],
            lora=["16;query,value,output"],
            quantize=["none"],
            checkpointing=True,
        ),
    output:
        fig="reports/figures/meltome_barplot.svg",
        stats="reports/tables/meltome_barplot_stats.tsv",
    notebook:
        "figures_meltome.py.ipynb"


rule meltome_hyperparameter:
    input:
        expand(
            config["meltome"]["predictions"],
            model=["8Me", "35Me", "150Me"],
            lora=[
                "16;query",
                "16;key",
                "16;value",
                "16;output",
                "16;query,output",
                "16;query,value,output",
                "32;query",
                "32;key",
                "32;value",
                "32;output",
                "32;query,output",
                "32;query,value,output",
                "64;query",
                "64;key",
                "64;value",
                "64;output",
                "64;query,output",
                "64;query,value,output",
            ],
            quantize=["none"],
            checkpointing=True,
            head=["nn"],
        ),
    output:
        fig="reports/figures/meltome_hyperparameter.svg",
        stats="reports/tables/meltome_hyperparameter_stats.tsv",
    notebook:
        "figures_meltome_hyperparameter.py.ipynb"


rule meltome_scatter:
    input:
        scatter=f"reports/figures/meltome_scatter_test_{finetune_wildcards}_head=nn.svg".format(
            model="{model}",
            lora="16;query,value,output",
            quantize="none",
            checkpointing=True,
        ),
    output:
        scatter="reports/figures/meltome_scatter_test_{model}_lora.svg",
    shell:
        'cp "{input.scatter}" {output.scatter}'


rule all_meltome:
    input:
        expand(
            f"reports/figures/meltome_scatter_test_{finetune_wildcards}_head={{head}}.svg",
            model=["1ve1"],
            lora=["16;query,value,output"],
            # lora=["none"],
            quantize=["none"],
            checkpointing=True,
            head=["nn"],
        ),